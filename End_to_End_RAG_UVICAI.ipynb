{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Finlay-J/PDF-RAG/blob/main/End_to_End_RAG_UVICAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File QA Rag Chatbot App with Gemini\n",
        "\n",
        "Following features:\n",
        "1. PDF document upload and indexing\n",
        "2. RAG system for query analysis and response\n",
        "3. Result streaming capabilities\n",
        "4. Show sources"
      ],
      "metadata": {
        "id": "zwkPDbkWBEOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip install"
      ],
      "metadata": {
        "id": "aUNdKlH80BfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.12\n",
        "!pip install sentence-transformers   # includes Hugging Face + Accelerate\n",
        "!pip install langchain_openai==0.0.8\n",
        "!pip install langchain-google-genai==0.0.8\n",
        "!pip install langchain_community==0.0.29\n",
        "!pip install streamlit==1.32.2\n",
        "!pip install PyMuPDF==1.24.0\n",
        "!pip install chromadb==0.4.24\n",
        "!pip install pyngrok==7.1.5\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "jnFdOt2VFZET",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put API key in YAML file"
      ],
      "metadata": {
        "id": "3d9sKwuN0EdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "import yaml, pathlib\n",
        "gemini_key  = userdata.get(\"GeminiAPI\")   # what you already called key\n",
        "\n",
        "# build the structure you want in the file\n",
        "secrets_dict = {\n",
        "    \"gemini\":  {\"api_key\": gemini_key},\n",
        "}\n",
        "\n",
        "# save it to disk\n",
        "with open(\"secrets.yaml\", \"w\") as f:\n",
        "    yaml.safe_dump(secrets_dict, f, sort_keys=False)"
      ],
      "metadata": {
        "id": "8R6bx96w7NSw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write app.py"
      ],
      "metadata": {
        "id": "nTL9ISiq0JSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "### Imports\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import yaml, pathlib\n",
        "\n",
        "# imports I used to deal with image-based pdfs, won't be used in codealong\n",
        "import pytesseract, io\n",
        "from PIL import Image\n",
        "import fitz\n",
        "\n",
        "\n",
        "### API key ###\n",
        "api_key = yaml.safe_load(pathlib.Path(\"secrets.yaml\").read_text())[\"gemini\"][\"api_key\"]\n",
        "# alternatively, api_key = [insert API key here]\n",
        "\n",
        "### USER MODIFIABLE CODE ###\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\") # Caches the object returned for 1 hr\n",
        "def configure_retriever(uploaded_files):\n",
        "  ''' Takes uploaded pdfs, creates document chunks, computes embeddings\n",
        "      Stores document chunks and embeddings into a Vector DB\n",
        "      Returns a retriever which can look up the vector Db\n",
        "      to return documents based on user input\n",
        "      Stores this in cache\n",
        "\n",
        "      Known errors: some pdfs are image based. Implementation followed today will\n",
        "      not cover how to deal with those, though it's not extraordinarily difficult.\n",
        "      It's one extra step between step 1 and step 2.\n",
        "  '''\n",
        "\n",
        "  # Step 1: load files\n",
        "  docs = []\n",
        "  temp_dir = tempfile.TemporaryDirectory()\n",
        "  for file in uploaded_files:\n",
        "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "    with open(temp_filepath, \"wb\") as f:\n",
        "      f.write(file.getvalue())\n",
        "\n",
        "      #if its a img no worky\n",
        "      loader = PyMuPDFLoader(temp_filepath)\n",
        "      docs.extend(loader.load())\n",
        "\n",
        "  # Step 2: Split files into chunks\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
        "  docs_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "  # Step 3: Vectorize/embedd chunks\n",
        "  # BAAI/bge-base-en-v1.5\n",
        "  model_name = \"sentence-transformers/all-mpnet-base-v2\" #arbitrary model\n",
        "  embeddings_model = HuggingFaceEmbeddings(\n",
        "      model_name=model_name,\n",
        "      model_kwargs={\"device\" : \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "      encode_kwargs={\"batch_size\": 32, \"normalize_embeddings\": True},\n",
        "  )\n",
        "\n",
        "  # Step 4: Return VectorDB\n",
        "  vectordb = Chroma.from_documents(docs_chunks, embeddings_model)\n",
        "  retriever = vectordb.as_retriever() #implicitly calculate cosine similarities in backend when called\n",
        "  return retriever\n",
        "\n",
        "\n",
        "### Misc ###\n",
        "st.set_page_config(page_title=\"File QA Chatbot\")\n",
        "st.title(\"Welcome to File QA RAG Chatbot\")\n",
        "\n",
        "# Creates UI element to accept PDF uploads\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "    label=\"Upload PDF files\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "if not uploaded_files:\n",
        "  st.info(\"Please upload PDF documents to continue\")\n",
        "  st.stop()\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.1,\n",
        "    max_retries=2,\n",
        "    streaming=True,\n",
        "    google_api_key=api_key,\n",
        ")\n",
        "\n",
        "qa_template = \"\"\"\n",
        ".\n",
        "\n",
        "{context}\n",
        "\n",
        "question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "###  Streamlit helper classes: ###\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "  ''' manages live updates to a streamlit app's display by appending new text tokens\n",
        "      to an existing text stream and rendinering the updated text in markdown'''\n",
        "  def __init__(self, container, initial_text=\"\"):\n",
        "    self.container = container\n",
        "    self.text = initial_text\n",
        "\n",
        "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "    self.text += token\n",
        "    self.container.markdown(self.text)\n",
        "\n",
        "  def on_llm_end(self, response, **kwargs):\n",
        "    # If nothing streamed, display the final text now\n",
        "    if not self.text:\n",
        "      self.text = response.generations[0][0].text\n",
        "      self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "  ''' Callback handler which does some post-processing on LLM response\n",
        "    Used to post the top 3 document sources used by the LLm in RAG response.\n",
        "  '''\n",
        "  def __init__(self, msg: st.write):\n",
        "    BaseCallbackHandler.__init__(self)\n",
        "    self.msg = msg\n",
        "    self.sources = []\n",
        "\n",
        "  def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
        "    source_ids = []\n",
        "    for d in documents: # retrieved documents from retriever based on user query\n",
        "      metadata = {\n",
        "          \"source\": d.metadata[\"source\"],\n",
        "          \"page\": d.metadata[\"page\"],\n",
        "          \"content\": d.page_content[:200]\n",
        "      }\n",
        "\n",
        "      idx = (metadata[\"source\"], metadata[\"page\"])\n",
        "      if idx not in source_ids:\n",
        "        source_ids.append(idx)\n",
        "        self.sources.append(metadata)\n",
        "\n",
        "  def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "    if len(self.sources):\n",
        "      st.markdown(\"__Sources:__\" + \"\\n\")\n",
        "      st.dataframe(data=pd.DataFrame(self.sources[:3]),\n",
        "                   width=1000)\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") # based on user question get context docs\n",
        "          |\n",
        "        retriever\n",
        "          |\n",
        "        format_docs,\n",
        "        \"question\": itemgetter(\"question\") # user question\n",
        "    }\n",
        "      |\n",
        "    qa_prompt\n",
        "      |\n",
        "    gemini\n",
        "\n",
        ")\n",
        "\n",
        "# store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
        "\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "  streamlit_msg_history.add_ai_message(\"Please ask your question\")\n",
        "\n",
        "# render current messages from streamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# if user inputs a new prompt, display it and show response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  with st.chat_message(\"ai\"):\n",
        "    token_box = st.empty()\n",
        "    stream_handler = StreamHandler(token_box)\n",
        "    sources_box = st.container()\n",
        "    pm_handler = PostMessageHandler(sources_box)\n",
        "\n",
        "    qa_rag_chain.invoke(\n",
        "        {\"question\": user_prompt},\n",
        "        {\"callbacks\": [stream_handler, pm_handler]},\n",
        "    )"
      ],
      "metadata": {
        "id": "vqQkIwXeG9bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run App"
      ],
      "metadata": {
        "id": "PTvNkv-E4Nkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8989 &>.logs.txt &"
      ],
      "metadata": {
        "id": "gIjjl6-mHqp1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "from google.colab import userdata\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('ngrok_auth_token')\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "sjO3peZtZR9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}